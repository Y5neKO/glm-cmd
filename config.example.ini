# GLM-CMD Configuration File Example
# Copy this file to ~/.glm-cmd/config.ini and edit with your settings

# ============================================================================
# Required Settings
# ============================================================================

# API Key for Zhipu AI
# Get your API key from: https://bigmodel.cn
api_key="your_api_key_here"

# API endpoint (REQUIRED)
# Choose one of the following endpoints:
#
# For Coding tasks (recommended):
#   endpoint="https://open.bigmodel.cn/api/coding/paas/v4"
#
# For general tasks:
#   endpoint="https://open.bigmodel.cn/api/paas/v4"
#
# Note: The endpoint must be specified either here or via GLM_CMD_ENDPOINT
# environment variable. There is no default endpoint.
endpoint="https://open.bigmodel.cn/api/coding/paas/v4"

# ============================================================================
# Optional Settings
# ============================================================================

# Model name to use
# Options: glm-4.7, glm-4-plus, glm-4-flash, etc.
# Default: glm-4.7
model="glm-4.7"

# User custom prompt (optional)
# This prompt will be prefixed to every user input before sending to the API.
# It's useful for adding context, constraints, or specific instructions.
#
# Examples:
#   user_prompt="Always explain the command before showing it"
#   user_prompt="Generate bash commands only"
#   user_prompt="Use safe options only, never destructive commands"
#   user_prompt="Respond in Chinese"
#
# Leave empty if not needed:
#   user_prompt=""
#
# Default: (empty - no custom prompt)
user_prompt=""

# Conversation memory settings
# These settings control whether GLM-CMD remembers previous conversations.
#
# memory_enabled: Enable/disable conversation history memory (true/false)
#   - When enabled, GLM-CMD will remember recent conversations and include
#     them in API requests for better context
#   - Default: false (disabled)
#
# memory_rounds: Number of recent conversation rounds to remember (1-20)
#   - Each "round" consists of one user input and one AI response
#   - More rounds = better context but higher API usage
#   - Only takes effect when memory_enabled is true
#   - Default: 5
#
# Examples:
#   memory_enabled=true
#   memory_rounds=10
#
# Note: Conversation history is stored in ~/.glm-cmd/history.json
memory_enabled=false
memory_rounds=5

# Temperature parameter (0.0 - 2.0)
# Lower values (0.0 - 0.3): More focused and deterministic
# Medium values (0.4 - 0.8): Balanced creativity and consistency
# Higher values (0.9 - 2.0): More creative and random
# Default: 0.7
temperature=0.7

# Maximum tokens in the response
# Default: 2048
max_tokens=2048

# Request timeout in seconds
# Default: 30
timeout=30

# ============================================================================
# Endpoint Selection Guide
# ============================================================================

# Coding Endpoint (Recommended):
#   endpoint="https://open.bigmodel.cn/api/coding/paas/v4"
#
#   Use this endpoint for:
#   - Code generation and analysis
#   - Command-line tool recommendations
#   - Script creation and debugging
#   - Development-related tasks
#
# Standard Endpoint:
#   endpoint="https://open.bigmodel.cn/api/paas/v4"
#
#   Use this endpoint for:
#   - General natural language tasks
#   - Text processing and analysis
#   - Translation and summarization
#   - Non-coding related queries

# ============================================================================
# Notes
# ============================================================================

# 1. Configuration priority:
#    - Environment variables have the highest priority
#    - Configuration file is used if environment variables are not set
#    - Default values are used for optional settings only
#
# 2. Required settings:
#    - api_key: MUST be configured
#    - endpoint: MUST be configured
#
# 3. You can override any config file setting with environment variables:
#    export GLM_CMD_API_KEY="your_key"
#    export GLM_CMD_ENDPOINT="https://open.bigmodel.cn/api/coding/paas/v4"
#    export GLM_CMD_MODEL="glm-4-plus"
#    export GLM_CMD_USER_PROMPT="Always explain the command"
#    export GLM_CMD_TEMP="1.2"
#
# 4. The configuration file location:
#    - Default: ~/.glm-cmd/config.ini
#    - Custom: Set GLM_CMD_CONFIG environment variable
#
# 5. Use --verbose or -V flag to enable verbose output for debugging
